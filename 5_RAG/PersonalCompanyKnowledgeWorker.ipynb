{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b463ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import email\n",
    "from email import policy\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity.aio import ClientSecretCredential\n",
    "from azure.identity import InteractiveBrowserCredential,DeviceCodeCredential\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc42c147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import (\n",
    "    DirectoryLoader, \n",
    "    UnstructuredHTMLLoader,\n",
    "    PyPDFLoader,\n",
    "    Docx2txtLoader\n",
    ")\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "import httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fce4d93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from msgraph import GraphServiceClient\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b24e36f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67d71187",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOX_API_KEY  = os.getenv(\"AUTOX_API_KEY\")\n",
    "NTNET_USERNAME = (os.getenv(\"NTNET_USERNAME\") or \"\").strip()\n",
    "\n",
    "# 3) Set proxy bypass BEFORE creating HTTP clients\n",
    "os.environ[\"NO_PROXY\"] = \",\".join(filter(None, [\n",
    "    os.getenv(\"NO_PROXY\",\"\"),\n",
    "    \".autox.corp.amdocs.azr\",\n",
    "    \"chat.autox.corp.amdocs.azr\",\n",
    "    \"localhost\",\"127.0.0.1\"\n",
    "]))\n",
    "os.environ[\"no_proxy\"] = os.environ[\"NO_PROXY\"]\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "import httpx\n",
    "\n",
    "http_client = httpx.Client(\n",
    "    verify=r\"C:\\amdcerts.pem\",  # Use corporate certs\n",
    "    timeout=30.0\n",
    ")\n",
    "\n",
    "# Create async client too (if needed)\n",
    "async_http_client = httpx.AsyncClient(\n",
    "    verify=r\"C:\\amdcerts.pem\",\n",
    "    timeout=30.0\n",
    ")\n",
    "\n",
    "# 5) Create LLM with custom HTTP client\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=\"https://chat.autox.corp.amdocs.azr/api/v1/proxy\",\n",
    "    api_key=AUTOX_API_KEY,\n",
    "    azure_deployment=\"gpt-4o-128k\",\n",
    "    model=\"gpt-4o-128k\",\n",
    "    temperature=0.1,\n",
    "    openai_api_version=\"2024-08-01-preview\",\n",
    "    default_headers={\"username\": NTNET_USERNAME, \"application\": \"testing-proxyapi\"},\n",
    "    http_client=http_client,\n",
    "    http_async_client=async_http_client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f00737fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_TENANT_ID = os.getenv(\"AZURE_TENANT_ID\")\n",
    "AZURE_CLIENT_ID = os.getenv(\"AZURE_CLIENT_ID\")\n",
    "AZURE_CLIENT_SECRET = os.getenv(\"AZURE_CLIENT_SECRET_VALUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2832e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "credential = DeviceCodeCredential(\n",
    "    tenant_id=AZURE_TENANT_ID,\n",
    "    client_id=AZURE_CLIENT_ID\n",
    ")\n",
    "scopes = ['https://graph.microsoft.com/.default']\n",
    "graph_client = GraphServiceClient(credentials=credential, scopes=scopes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a193c32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing connection...\n",
      "To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code L6SQM7MRS to authenticate.\n"
     ]
    }
   ],
   "source": [
    "# try:\n",
    "#     print(\"Testing connection...\")\n",
    "    \n",
    "#     # This will work because you're authenticated as yourself\n",
    "#     user = await graph_client.me.get()\n",
    "#     print(f\"✓ Connected as: {user.display_name}\")\n",
    "#     print(f\"  Email: {user.mail or user.user_principal_name}\")\n",
    "    \n",
    "#     # Access YOUR OneNote notebooks directly\n",
    "#     notebooks = await graph_client.me.onenote.notebooks.get()\n",
    "#     if notebooks.value:\n",
    "#         print(f\"  Found {len(notebooks.value)} notebooks:\")\n",
    "#         for nb in notebooks.value:\n",
    "#             print(f\"    - {nb.display_name}\")\n",
    "#     else:\n",
    "#         print(\"  No notebooks found\")\n",
    "        \n",
    "# except Exception as e:\n",
    "#     print(f\"✗ Connection failed: {e}\")\n",
    "#     raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "395b3386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "def extract_and_describe_images(mht_path: str) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "        Extract images from MHT file and get their descriptions using Azure OpenAI\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping image CIDs to their data and descriptions\n",
    "    \"\"\"\n",
    "    images = {}\n",
    "\n",
    "    with open(mht_path, 'rb') as f:\n",
    "        msg = email.message_from_binary_file(f, policy=policy.default)\n",
    "    \n",
    "    image_count = 0\n",
    "    for part in msg.walk():\n",
    "        content_type = part.get_content_type()\n",
    "    \n",
    "        if content_type.startswith(\"image/\"):\n",
    "            image_count+=1\n",
    "\n",
    "            cid = part.get('Content-ID', '')\n",
    "            if cid:\n",
    "                cid = cid.strip('<>')\n",
    "            else:\n",
    "                # Use Content-Location as fallback\n",
    "                cid = part.get('Content-Location', f'image_{len(images)}')\n",
    "            \n",
    "            image_data = part.get_payload(decode=True)\n",
    "\n",
    "            if llm:\n",
    "                print(f\"  Analyzing image {image_count}...\")\n",
    "                description = describe_image(image_data, content_type)\n",
    "            \n",
    "            else:\n",
    "                description = \"[Image - description unavailable without Azure API]\"\n",
    "\n",
    "            images[cid] = {\n",
    "                'data': image_data,\n",
    "                'content_type': content_type,\n",
    "                'description': description,\n",
    "                'size': len(image_data)\n",
    "            }\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e42c0cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def describe_image(image_data: bytes, content_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Use Azure OpenAI to describe what's in the image\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Convert image to base64\n",
    "        base64_image = base64.b64encode(image_data).decode('utf-8')\n",
    "        \n",
    "        # Call Azure OpenAI with vision capabilities\n",
    "        response = llm.invoke(  # Your Azure deployment name\n",
    "            [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"\"\"Describe this image in detail for personal note-taking purposes. Include:\n",
    "- Any text visible in the image\n",
    "- Diagrams, charts, or visual elements\n",
    "- Key concepts or information shown\n",
    "- Any annotations or highlights\n",
    "Be concise but thorough.\"\"\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:{content_type};base64,{base64_image}\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return response.content\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"    ⚠️  Error describing image: {e}\")\n",
    "        return f\"[Image - error getting description: {str(e)[:100]}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7c26d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzing image 1...\n",
      "  Analyzing image 2...\n",
      "  Analyzing image 3...\n",
      "  Analyzing image 4...\n",
      "  Analyzing image 5...\n",
      "  Analyzing image 6...\n",
      "  Analyzing image 7...\n",
      "  Analyzing image 8...\n",
      "  Analyzing image 9...\n",
      "  Analyzing image 10...\n",
      "  Analyzing image 11...\n",
      "  Analyzing image 12...\n",
      "  Analyzing image 13...\n",
      "  Analyzing image 14...\n",
      "  Analyzing image 15...\n",
      "  Analyzing image 16...\n",
      "  Analyzing image 17...\n",
      "  Analyzing image 18...\n",
      "  Analyzing image 19...\n",
      "  Analyzing image 20...\n",
      "  Analyzing image 21...\n",
      "  Analyzing image 22...\n",
      "  Analyzing image 23...\n",
      "  Analyzing image 24...\n",
      "  Analyzing image 25...\n",
      "  Analyzing image 26...\n",
      "  Analyzing image 27...\n",
      "  Analyzing image 28...\n",
      "  Analyzing image 29...\n",
      "  Analyzing image 30...\n",
      "  Analyzing image 31...\n",
      "  Analyzing image 32...\n",
      "  Analyzing image 33...\n",
      "  Analyzing image 34...\n",
      "  Analyzing image 35...\n",
      "  Analyzing image 36...\n",
      "  Analyzing image 37...\n",
      "  Analyzing image 38...\n",
      "  Analyzing image 39...\n",
      "  Analyzing image 40...\n",
      "  Analyzing image 41...\n",
      "  Analyzing image 42...\n",
      "  Analyzing image 43...\n",
      "  Analyzing image 44...\n",
      "  Analyzing image 45...\n",
      "  Analyzing image 46...\n",
      "  Analyzing image 47...\n",
      "  Analyzing image 48...\n",
      "  Analyzing image 49...\n",
      "  Analyzing image 50...\n",
      "  Analyzing image 51...\n",
      "  Analyzing image 52...\n",
      "  Analyzing image 53...\n",
      "  Analyzing image 54...\n",
      "  Analyzing image 55...\n",
      "  Analyzing image 56...\n",
      "  Analyzing image 57...\n",
      "  Analyzing image 58...\n",
      "  Analyzing image 59...\n",
      "  Analyzing image 60...\n",
      "  Analyzing image 61...\n",
      "  Analyzing image 62...\n",
      "  Analyzing image 63...\n",
      "  Analyzing image 64...\n",
      "  Analyzing image 65...\n",
      "  Analyzing image 66...\n",
      "  Analyzing image 67...\n",
      "  Analyzing image 68...\n",
      "  Analyzing image 69...\n",
      "  Analyzing image 70...\n",
      "  Analyzing image 71...\n",
      "  Analyzing image 72...\n",
      "  Analyzing image 73...\n",
      "  Analyzing image 74...\n",
      "  Analyzing image 75...\n",
      "  Analyzing image 76...\n",
      "  Analyzing image 77...\n",
      "  Analyzing image 78...\n",
      "  Analyzing image 79...\n",
      "  Analyzing image 80...\n",
      "  Analyzing image 81...\n",
      "  Analyzing image 82...\n",
      "  Analyzing image 83...\n",
      "  Analyzing image 84...\n",
      "  Analyzing image 85...\n",
      "  Analyzing image 86...\n",
      "  Analyzing image 87...\n",
      "  Analyzing image 88...\n",
      "  Analyzing image 89...\n",
      "  Analyzing image 90...\n",
      "  Analyzing image 91...\n",
      "  Analyzing image 92...\n",
      "  Analyzing image 93...\n",
      "  Analyzing image 94...\n",
      "  Analyzing image 95...\n",
      "  Analyzing image 96...\n",
      "  Analyzing image 97...\n",
      "  Analyzing image 98...\n",
      "  Analyzing image 99...\n",
      "  Analyzing image 100...\n",
      "  Analyzing image 101...\n",
      "  Analyzing image 102...\n",
      "  Analyzing image 103...\n",
      "  Analyzing image 104...\n",
      "  Analyzing image 105...\n",
      "  Analyzing image 106...\n",
      "  Analyzing image 107...\n",
      "  Analyzing image 108...\n",
      "  Analyzing image 109...\n",
      "  Analyzing image 110...\n",
      "  Analyzing image 111...\n",
      "  Analyzing image 112...\n",
      "  Analyzing image 113...\n",
      "  Analyzing image 114...\n",
      "  Analyzing image 115...\n",
      "  Analyzing image 116...\n",
      "  Analyzing image 117...\n",
      "  Analyzing image 118...\n",
      "  Analyzing image 119...\n",
      "  Analyzing image 120...\n",
      "  Analyzing image 121...\n",
      "  Analyzing image 122...\n",
      "  Analyzing image 123...\n",
      "  Analyzing image 124...\n",
      "  Analyzing image 125...\n",
      "  Analyzing image 126...\n",
      "  Analyzing image 127...\n",
      "  Analyzing image 128...\n",
      "  Analyzing image 129...\n",
      "  Analyzing image 130...\n",
      "  Analyzing image 131...\n",
      "  Analyzing image 132...\n",
      "  Analyzing image 133...\n",
      "  Analyzing image 134...\n",
      "  Analyzing image 135...\n",
      "  Analyzing image 136...\n",
      "  Analyzing image 137...\n",
      "  Analyzing image 138...\n",
      "  Analyzing image 139...\n",
      "  Analyzing image 140...\n",
      "  Analyzing image 141...\n",
      "  Analyzing image 142...\n",
      "  Analyzing image 143...\n",
      "  Analyzing image 144...\n",
      "  Analyzing image 145...\n",
      "  Analyzing image 146...\n",
      "  Analyzing image 147...\n",
      "  Analyzing image 148...\n",
      "  Analyzing image 149...\n",
      "  Analyzing image 150...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'1'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m absolute_path = os.path.abspath(knowledge_base_path)\n\u001b[32m      3\u001b[39m images = extract_and_describe_images(absolute_path)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[31mKeyError\u001b[39m: '1'"
     ]
    }
   ],
   "source": [
    "knowledge_base_path = \"knowledge-base/amdocsKnowledgeBase/Company.mht\"\n",
    "absolute_path = os.path.abspath(knowledge_base_path)\n",
    "images = extract_and_describe_images(absolute_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90bec985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 150 images to cache\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('knowledge-base/amdocsKnowledgeBase/images_cache.pkl', 'wb') as f:\n",
    "    pickle.dump(images, f)\n",
    "print(f\"Saved {len(images)} images to cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218d4759",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cache_file = 'knowledge-base/amdocsKnowledgeBase/images_cache.pkl'\n",
    "\n",
    "# Check if cache exists\n",
    "if os.path.exists(cache_file):\n",
    "    print(\"Loading images from cache...\")\n",
    "    with open(cache_file, 'rb') as f:\n",
    "        images = pickle.load(f)\n",
    "    print(f\"Loaded {len(images)} images from cache\")\n",
    "else:\n",
    "    print(\"No cache found. Processing images...\")\n",
    "    # Your extract_and_describe_images() call here\n",
    "    images = extract_and_describe_images(absolute_path)\n",
    "    \n",
    "    # Save to cache\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(images, f)\n",
    "    print(f\"Processed and cached {len(images)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cffc0870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Description of the Image:\n",
      "\n",
      "#### **Section 1: Tunneling Diagram**\n",
      "- **Title:** \"Tunneling\"\n",
      "- **Visual Elements:**\n",
      "  - A diagram illustrating the process of port forwarding.\n",
      "  - Three options at the top: \n",
      "    - \"Local port forwarding\" (selected)\n",
      "    - \"Remote port forwarding\"\n",
      "    - \"Dynamic port forwarding (SOCKS proxy)\"\n",
      "  - Left side: \"My computer with MobaXterm\" connected to \"Local clients.\"\n",
      "  - Middle: A firewall with a flame icon, showing port 6432.\n",
      "  - Right side: \"Remote server\" with a lock icon and SSH server.\n",
      "  - Arrows indicate the flow of data from local clients to the remote server via port 6432.\n",
      "- **Text in Diagram:**\n",
      "  - \"svod-nehash-postgre\" and \"6432\" near the remote server.\n",
      "  - \"100.66.122.185\" and \"22\" near the SSH server.\n",
      "  - \"Local clients can access the remote server by connecting to <mycomputer>:6432.\"\n",
      "- **Buttons:** \"Save\" and \"Cancel\" at the bottom.\n",
      "\n",
      "#### **Section 2: Configuration Details**\n",
      "- **Text:**\n",
      "  - \"Port: 6432\"\n",
      "  - \"Consul ip: 100.66.122.185\"\n",
      "  - \"Remote server\"\n",
      "  - \"db url: svod-nehash-postgres-svo.c3v7xmzz7djr.eu-west-1.rds.amazonaws.com\"\n",
      "\n",
      "#### **Section 3: MobaXterm Interface**\n",
      "- **Title:** \"Welcome to MobaSSHTunnel - Graphical port forwarding tool\"\n",
      "- **Table Columns:**\n",
      "  - \"Name,\" \"Type,\" \"Start/Stop,\" \"Forward port,\" \"Destination server,\" \"SSH server,\" and \"Settings.\"\n",
      "- **Row Details:**\n",
      "  - Name: \"green-dev\"\n",
      "  - Type: Local\n",
      "  - Forward Port: 6432\n",
      "  - Destination Server: \"rds-svo.c3v7xmzz7djr.eu-west-1.rds.am\"\n",
      "  - SSH Server: \"oas@100.66.122.185:22\"\n",
      "- **Highlighted Element:**\n",
      "  - A yellow arrow pointing to a button in the \"Settings\" column, indicating the addition of a `.pem` key.\n",
      "\n",
      "#### **Key Notes:**\n",
      "- \"Key add .pem key to start connection\" is mentioned as an instruction.\n"
     ]
    }
   ],
   "source": [
    "print(images[\"file:///C:/A5099239/Company_files/image002.png\"][\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "27e61bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List,Any\n",
    "def create_enriched_chunks(mht_path: str,images: Dict[str, Dict]) -> List[Dict[str, Any]]:\n",
    "    chunks = []\n",
    "    with open(mht_path, 'rb') as f:\n",
    "        msg = email.message_from_binary_file(f, policy=policy.default)\n",
    "    \n",
    "    for part in msg.walk():\n",
    "        if part.get_content_type() == \"text/html\":\n",
    "            html_content = part.get_content();\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "            sections = soup.find_all(['div','sections','article'])\n",
    "            for idx,section in enumerate(sections):\n",
    "                text = section.get_text(separator='\\n', strip=True)\n",
    "\n",
    "                if not text or not text.strip():\n",
    "                    continue\n",
    "\n",
    "                section_images = []\n",
    "\n",
    "                for img in section.find_all('img'):\n",
    "                    img_src = img.get('src','')\n",
    "\n",
    "                    if img_src:\n",
    "                        filename = img_src.split('/')[-1]\n",
    "                    \n",
    "                    matching_key = None\n",
    "                    for key in images.keys():\n",
    "                        if filename in key:\n",
    "                            matching_key = key\n",
    "                            break\n",
    "                    \n",
    "                    if matching_key:\n",
    "                        section_images.append({\n",
    "                            'cid': matching_key,\n",
    "                            'alt': img.get('alt', ''),\n",
    "                            'description': images[matching_key]['description']\n",
    "                        })\n",
    "            \n",
    "                section_links = []\n",
    "\n",
    "                for a_tag in section.find_all('a', href=True):\n",
    "                    link_text = a_tag.get_text(strip=True)\n",
    "                    link_url = a_tag['href']\n",
    "\n",
    "                    if link_url and not link_url.startswith('cid:'):\n",
    "                        section_links.append({\n",
    "                            'text': link_text,\n",
    "                            'url': link_url,\n",
    "                            'title': a_tag.get('title','')\n",
    "                        })\n",
    "\n",
    "                enriched_text = f\"Content: {text}\\n\\n\"\n",
    "\n",
    "                if section_images:\n",
    "                    enriched_text += \"Images in this section:\\n\"\n",
    "                    for img_info in section_images:\n",
    "                        # Extract just the filename from the full path for readability\n",
    "                        filename = img_info['cid'].split('/')[-1]  # Gets \"image002.png\" from full path\n",
    "                        \n",
    "                        enriched_text += f\"- Image: {filename}\\n\"\n",
    "                        enriched_text += f\"  Description: {img_info['description']}\\n\"\n",
    "                        if img_info['alt']:\n",
    "                            enriched_text += f\"  Alt text: {img_info['alt']}\\n\"\n",
    "                    enriched_text += \"\\n\"\n",
    "                \n",
    "                if section_links:\n",
    "                    enriched_text += \"Links in this section:\\n\"\n",
    "                    for link_info in section_links:\n",
    "                        enriched_text += f\"- Link: {link_info['text']}\\n\"\n",
    "                        enriched_text += f\"  URL: {link_info['url']}\\n\"\n",
    "                        if link_info['title']:\n",
    "                            enriched_text += f\"  Title: {link_info['title']}\\n\"\n",
    "\n",
    "                chunks.append({\n",
    "                    'text': enriched_text,\n",
    "                    'raw_text': text,\n",
    "                    'metadata': {\n",
    "                        'source': str(mht_path),\n",
    "                        'chunk_id': idx,\n",
    "                        'images': section_images,\n",
    "                        'links': section_links,\n",
    "                        'has_images': len(section_images) > 0,\n",
    "                        'has_links': len(section_links) > 0\n",
    "                    }\n",
    "                })\n",
    "    return chunks\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70aecfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"C:/AgenticAI/all-MiniLM-L6-v2\",  # Your local model path\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ab466f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Content: GA To GA\\nThursday,\\nJune 5, 2025\\n12:29\\nPM\\nPlatform release\\nversion 25.03:\\nPlatform\\nRelease 25.03 - Open Network Platform - Confluence AT - Production\\nBlueIngressHost:\\nportal-ga-vishnuna.oso.corp.amdocs.aws\\n\\nLinks in this section:\\n- Link: Platform\\nRelease 25.03 - Open Network Platform - Confluence AT - Production\\n  URL: https://confluence/display/ONPONE/Platform+Release+25.03\\n', 'raw_text': 'GA To GA\\nThursday,\\nJune 5, 2025\\n12:29\\nPM\\nPlatform release\\nversion 25.03:\\nPlatform\\nRelease 25.03 - Open Network Platform - Confluence AT - Production\\nBlueIngressHost:\\nportal-ga-vishnuna.oso.corp.amdocs.aws', 'metadata': {'source': 'c:\\\\AgenticAI\\\\llm_engineering\\\\week5\\\\knowledge-base\\\\amdocsKnowledgeBase\\\\Company.mht', 'chunk_id': 0, 'images': [], 'links': [{'text': 'Platform\\nRelease 25.03 - Open Network Platform - Confluence AT - Production', 'url': 'https://confluence/display/ONPONE/Platform+Release+25.03', 'title': ''}], 'has_images': False, 'has_links': True}}\n"
     ]
    }
   ],
   "source": [
    "chunks = create_enriched_chunks(absolute_path, images)\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64727f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'knowledge-base/amdocsKnowledgeBase/knowledge_base_db'\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "40d611ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "def add_to_vector_store(chunks: List[Dict[str, Any]]):\n",
    "    print(f\"Adding {len(chunks)} chunks to vector store...\")\n",
    "    documents = []\n",
    "    for chunk in chunks:\n",
    "        # Generate embedding from the enriched text\n",
    "        doc = Document(\n",
    "            page_content=chunk['text'],  # The enriched text for embedding\n",
    "            metadata={\n",
    "                'source': chunk['metadata']['source'],\n",
    "                'chunk_id': chunk['metadata']['chunk_id'],\n",
    "                'has_images': chunk['metadata']['has_images'],\n",
    "                'has_links': chunk['metadata']['has_links'],\n",
    "                'num_images': len(chunk['metadata']['images']),\n",
    "                'num_links': len(chunk['metadata']['links']),\n",
    "                # Store complex objects as JSON strings\n",
    "                'links_json': json.dumps(chunk['metadata']['links']),\n",
    "                'images_json': json.dumps(chunk['metadata']['images']),\n",
    "                'raw_text': chunk['raw_text']  # Optional: keep original text\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "471d34bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 330 chunks to vector store...\n"
     ]
    }
   ],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    documents=add_to_vector_store(chunks),\n",
    "    embedding=embeddings,\n",
    "    persist_directory=db_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5b7a6723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 330 vectors with 384 dimensions in the vector store\n"
     ]
    }
   ],
   "source": [
    "collection = vectorstore._collection\n",
    "count = collection.count()\n",
    "\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"There are {count:,} vectors with {dimensions:,} dimensions in the vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2d0a7a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a27c4d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import StdOutCallbackHandler\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 40})\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "56f0e48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To set up an AWS 100K environment, follow these steps based on the provided context:\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 1: EKS Cluster Setup**\n",
      "1. **Create an EKS Cluster:**\n",
      "   - Use the Jenkins pipeline for EKS setup:\n",
      "     - [EKS Pipeline](https://jenkins-devops.neo.corp.amdocs.aws/job/DevOps/job/eks-poc-multibranch/job/master/)\n",
      "   - Ensure the cluster name follows the format `clustername-username`.\n",
      "\n",
      "2. **Parameters for EKS Cluster:**\n",
      "   - Set the following parameters:\n",
      "     - `eks_clustername`: Name of the cluster.\n",
      "     - `aws_region`: AWS region (e.g., `eu-west-1`).\n",
      "     - `default_no_of_nodes`: 8 nodes.\n",
      "     - `kube_version`: 1.29.\n",
      "     - `node_instance_type`: `m6i.4xlarge`.\n",
      "     - `spot`: Set to `false` for on-demand nodes.\n",
      "\n",
      "3. **Verify Auto Scaling Groups:**\n",
      "   - Ensure the cluster has 3 auto-scaling groups created.\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 2: MS Automation**\n",
      "1. **Deploy Middleware Services:**\n",
      "   - Use the Jenkins pipeline for MS automation:\n",
      "     - [MS Automation Pipeline](https://jenkins-devops.neo.corp.amdocs.aws/job/DevOps/job/aws-ms-automation-poc/job/master/)\n",
      "   - Deploy services like Kafka, Cassandra, Consul, Elasticsearch, and Prometheus.\n",
      "\n",
      "2. **RDS Creation:**\n",
      "   - Create the RDS database with the following parameters:\n",
      "     - `rds_db_user`: `postgres`.\n",
      "     - `rds_db_password`: `postgres`.\n",
      "     - `RDSASMDBEndpoint`: Provide the security RDS URL for DR environments.\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 3: Chart Deployment**\n",
      "1. **Deploy Helm Charts:**\n",
      "   - Use the Jenkins pipeline for chart deployment:\n",
      "     - [Chart Deployment Pipeline](https://jenkins-devops.neo.corp.amdocs.aws/job/SVO/job/utilities/job/oso-aws-hybrid-pipeline-deploy-green/1144/)\n",
      "   - Provide the following parameters:\n",
      "     - `helm_url`: URL of the Helm chart (e.g., `https://nexus-proxy.neo.corp.amdocs.aws/...`).\n",
      "     - `tests_url`: URL of the test values YAML file.\n",
      "     - `release_to_deploy`: Specify the release version (e.g., `master`).\n",
      "\n",
      "2. **Enable Optional Features:**\n",
      "   - Enable `service_mesh_enabled` if required.\n",
      "   - Configure `keycloak` if needed for authentication.\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 4: NFT Post Installation**\n",
      "1. **Run NFT Tests:**\n",
      "   - Use the Jenkins pipeline for NFT post-installation:\n",
      "     - [NFT Post Installation Pipeline](https://jenkins-devops.neo.corp.amdocs.aws/job/SVO/job/utilities/job/aws-nft-post-installation/)\n",
      "   - Set the `test_type` parameter to `test_100k`.\n",
      "\n",
      "2. **Validate Deployment:**\n",
      "   - Ensure all services are running and validate the environment for 100K load.\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 5: Environment Validation**\n",
      "1. **Check Resource Allocation:**\n",
      "   - Verify node allocation and resource limits using `kubectl` commands.\n",
      "   - Ensure no pods are in a `Failed` or `Pending` state.\n",
      "\n",
      "2. **Run Stability Tests:**\n",
      "   - Refer to the [Sizing Details for Stability Test](https://confluence/display/DOPNEW/Sizing+Details+For+Stability+Test+-+AWS) for 100K environments.\n",
      "\n",
      "3. **Monitor Logs:**\n",
      "   - Use tools like `k9s` or `kubectl logs` to monitor application and system logs.\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 6: Environment Maintenance**\n",
      "1. **Start/Stop Environment:**\n",
      "   - Use the Jenkins job for starting/stopping resources:\n",
      "     - [Start/Stop Job](https://jenkins-devops.neo.corp.amdocs.aws/job/SVO/job/utilities/job/AWSResourceStartStop)\n",
      "\n",
      "2. **Delete Environment:**\n",
      "   - Use the Jenkins job for deleting CloudFormation stacks:\n",
      "     - [Delete Environment Job](https://jenkins-devops.neo.corp.amdocs.aws/job/DevOps/job/aws-jobs/job/delete-cloudformation-stacks-aws/)\n",
      "\n",
      "---\n",
      "\n",
      "### **Additional Notes:**\n",
      "- Ensure all configurations are aligned with the `values.yaml` and `override-values.yaml` files.\n",
      "- Use the `hostUrl` parameter for shared Kubernetes environments.\n",
      "- For disaster recovery (DR) setups, configure secondary site parameters.\n",
      "\n",
      "By following these steps, you can successfully set up and validate an AWS 100K environment.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the steps to setup AWS 100K env\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "96be99a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "def chat(question, history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "172b9caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
