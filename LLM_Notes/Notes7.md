# RAG (Retrieval Augmented Generation) - Study Notes

## ğŸ“š Table of Contents
1. [LLM Types](#llm-types)
2. [Vector Embeddings](#vector-embeddings)
3. [RAG Architecture](#rag-architecture)
4. [Document Processing Pipeline](#document-processing-pipeline)
5. [LangChain Components](#langchain-components)
6. [Optimization Strategies](#optimization-strategies)

---

## ğŸ¤– LLM Types

### Auto-Regressive LLMs
- **How it works**: Predicts the **next token** based on **previous tokens**
- **Examples**: GPT-3, GPT-4, Claude
- **Use cases**: Text generation, chat, code completion

### Auto-Encoding LLMs
- **How it works**: Produces output based on the **full input** (bidirectional understanding)
- **Examples**: BERT, OpenAI Embeddings
- **Use cases**: 
  - Sentiment analysis
  - Text classification
  - **Vector embeddings generation** â­

---

## ğŸ§® Vector Embeddings

### What are Vectors?
> Vectors mathematically represent the **semantic meaning** of text as a list of numbers

**Key Properties:**
- **Dimensions**: Typically 384 to 3072 dimensions
- **Semantic similarity**: Similar inputs â†’ Similar vectors (close in vector space)
- **Mathematical operations**: Supports algebraic operations

### Vector Math Example:
```
Vector["King"] - Vector["Man"] + Vector["Woman"] â‰ˆ Vector["Queen"]
```

### Popular Embedding Models:
| Model | Provider | Dimensions | Use Case |
|-------|----------|------------|----------|
| Word2Vec | Google | 300 | Traditional NLP |
| BERT | Google | 768 | General purpose |
| OpenAI Embeddings | OpenAI | 1536 | High accuracy |
| all-MiniLM-L6-v2 | HuggingFace | 384 | Fast & efficient |

---

## ğŸ—ï¸ RAG Architecture

### High-Level Flow:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         RAG PIPELINE                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User Question
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vectorize      â”‚  Convert question to vector embedding
â”‚  Question       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Retrieve       â”‚  Find k most similar document chunks
â”‚  from Vector DB â”‚  (Semantic search)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Build Prompt   â”‚  Question + Retrieved Context
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM Generate   â”‚  Generate answer based on context
â”‚  Answer         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
     Response
```

---

## ğŸ“„ Document Processing Pipeline

### Step 1: Load Documents
```python
from langchain_community.document_loaders import DirectoryLoader, TextLoader

# Load all documents from folder
loader = DirectoryLoader('./docs', glob="**/*.txt", loader_cls=TextLoader)
documents = loader.load()
```

### Step 2: Add Metadata
```python
# Add source, date, or other metadata
for doc in documents:
    doc.metadata['source'] = doc.metadata['source']
    doc.metadata['processed_date'] = datetime.now()
```

### Step 3: Split into Chunks
```python
from langchain.text_splitter import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    chunk_size=1000,      # 1000 characters per chunk
    chunk_overlap=200     # 200 characters overlap
)
chunks = text_splitter.split_documents(documents)
```

### Why Chunk Overlap?
```
Chunk 1: [===================>          ]
                           â†“ Overlap
Chunk 2:                [===================>    ]
```
**Benefit**: Ensures context continuity across chunk boundaries

### Visual Example:
```
Original Document (2500 chars):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ The quick brown fox jumps over the lazy dog...        â”‚
â”‚ [continues for 2500 characters]                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

After Chunking (size=1000, overlap=200):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chunk 1          â”‚ (chars 0-1000)
â”‚ [0â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€1000]â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ Chunk 2      â”‚ (chars 800-1800)
                   â”‚ [800â”€â”€â”€â”€1800]â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                 â”‚ Chunk 3      â”‚ (chars 1600-2500)
                                 â”‚ [1600â”€â”€â”€2500]â”‚
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ LangChain Components

### Key Abstractions:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LangChain RAG Stack                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚    LLM     â”‚    â”‚ Retriever  â”‚    â”‚  Memory    â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚ GPT-4      â”‚    â”‚ Vector DB  â”‚    â”‚ Chat       â”‚    â”‚
â”‚  â”‚ Claude     â”‚â—„â”€â”€â”€â”¤ Search     â”‚â—„â”€â”€â”€â”¤ History    â”‚    â”‚
â”‚  â”‚ Llama      â”‚    â”‚ Top-k      â”‚    â”‚ Context    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1. LLM (Language Model)
- The brain that generates answers
- Examples: GPT-4, Claude, Llama

### 2. Retriever
- **Purpose**: Fetches relevant document chunks from vector store
- **How**: Converts query to vector â†’ Finds k nearest neighbors
- **Output**: List of relevant text chunks

### 3. Memory
- **Purpose**: Maintains conversation context
- **Types**: 
  - `ConversationBufferMemory` - Stores all messages
  - `ConversationSummaryMemory` - Summarizes old messages

### Putting It Together:
```python
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_core.callbacks import StdOutCallbackHandler

# Setup memory
memory = ConversationBufferMemory(
    memory_key='chat_history',
    return_messages=True
)

# Setup retriever
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

# Create conversation chain
conversation_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    callbacks=[StdOutCallbackHandler()]  # Debug: shows all prompts
)

# Use it
result = conversation_chain.invoke({"question": "What is RAG?"})
print(result["answer"])
```

---

## ğŸ¯ Vector Datastores

### Popular Options:

| Database | Type | Best For | Local/Cloud |
|----------|------|----------|-------------|
| **Chroma** | Open source | Development, small scale | Local |
| **FAISS** | Facebook AI | High performance | Local |
| **Pinecone** | Managed | Production, scale | Cloud |
| **Weaviate** | Open source | Production | Both |
| **Qdrant** | Open source | Production | Both |

### Example with Chroma:
```python
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

# Create embeddings
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# Create vector store
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./chroma_db"
)
```

---

## ğŸš€ Optimization Strategies

### Problem: Wrong Chunks Retrieved
```
User asks: "How to deploy to production?"
Retrieved: Chunks about development setup âŒ
```

### Solution Approaches:

#### 1. **Adjust Chunk Size**
```python
# Too large â†’ Less precise
text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=200)

# Too small â†’ Loss of context
text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=50)

# Sweet spot (depends on use case)
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
```

#### 2. **Increase Chunk Overlap**
```python
# More overlap = Better context continuity
text_splitter = CharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=300  # 30% overlap
)
```

#### 3. **Retrieve More Chunks**
```python
# Default: k=4 chunks
retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

# More context: k=25 chunks (more tokens, better context)
retriever = vectorstore.as_retriever(search_kwargs={"k": 25})

# Balance: k=10 chunks
retriever = vectorstore.as_retriever(search_kwargs={"k": 10})
```

#### 4. **Use Full Documents**
Instead of chunking, store entire related documents:
```python
# For smaller, coherent documents
# Don't chunk - use whole document
vectorstore.add_documents(full_documents)
```

#### 5. **Hybrid Search**
Combine semantic search with keyword search:
```
Vector Search (semantic)  +  BM25 (keyword)  =  Better Results
```

---

## ğŸ” Debugging RAG with Callbacks

### Using StdOutCallbackHandler:
```python
from langchain_core.callbacks import StdOutCallbackHandler

conversation_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    callbacks=[StdOutCallbackHandler()]  # Shows all internal prompts
)
```

**Output Example:**
```
> Entering new ConversationalRetrievalChain chain...
> Entering new LLMChain chain...
Prompt: Use the following pieces of context to answer...
Context: [Retrieved chunks shown here]
Question: What is RAG?
> Finished chain.
```

### Common Problems & Fixes:

| Problem | Diagnosis | Solution |
|---------|-----------|----------|
| Wrong chunks retrieved | Check retrieved chunks in callback | Adjust chunking strategy |
| Too generic answers | Not enough context | Increase k value |
| Hallucinations | No relevant chunks found | Improve document coverage |
| Slow responses | Too many chunks | Reduce k value |

---

## ğŸ“Š RAG Performance Tuning

### Key Metrics:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         RAG Optimization Goals          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  Relevance â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  (Right chunks)   â”‚                     â”‚
â”‚                   â–¼                     â”‚
â”‚  Accuracy â”€â”€â”€â”€â”€â”€â”€â–º Performance         â”‚
â”‚  (Correct answer) â”‚                     â”‚
â”‚                   â–²                     â”‚
â”‚  Speed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚  (Fast response)                        â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tuning Parameters:

| Parameter | Impact | Recommendation |
|-----------|--------|----------------|
| `chunk_size` | Context per chunk | 500-1500 chars |
| `chunk_overlap` | Context continuity | 10-20% of chunk_size |
| `k` (retrieval) | Context amount | 5-25 chunks |
| `temperature` | Answer creativity | 0.0-0.3 for factual |

---

## ğŸ’¡ Key Takeaways

1. **RAG = Retrieval + Generation**: Combines document search with LLM generation
2. **Embeddings are crucial**: Quality embeddings = Better retrieval
3. **Chunking strategy matters**: Balance between context and precision
4. **Iterate and optimize**: Use callbacks to debug and improve
5. **It's not magic**: RAG is just vector search + prompt engineering

---

## ğŸ“ Next Steps

- Experiment with different chunk sizes
- Try various embedding models
- Implement hybrid search
- Add metadata filtering
- Build custom retrievers
- Monitor and improve retrieval accuracy

