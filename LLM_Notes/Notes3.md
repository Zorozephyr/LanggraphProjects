# ğŸ”¤ Tokenization & Model Variants

## ğŸ“š What is a Tokenizer?

Converts text â†” token IDs. Models only understand numbers, not text.

```
Text Input â†’ [Tokenizer] â†’ Token IDs â†’ LLM Model
```

**Why it matters:**
- Models need token IDs (not text)
- Each model has its own tokenizer
- Tokenizer affects context window size
- Special tokens control model behavior

---

## ğŸ§  Vocabulary & Special Tokens

**Vocabulary:** Mapping of tokens to IDs
```
Token 101 â†’ "[CLS]"    Token 1045 â†’ "hello"
Token 102 â†’ "[SEP]"    Token 2054 â†’ "world"
```

**Special Tokens by Model:**
```
BERT:   [CLS], [SEP], [PAD], [UNK], [MASK]
GPT:    <bos>, <eos>, <pad>, <unk>
Llama:  <bos>, <eos>, <pad>, <unk>
```

---

## ğŸ”¤ Tokenization Methods

| Method | Example | Pros | Cons |
|--------|---------|------|------|
| **Character** | "hello" â†’ ['h','e','l','l','o'] | Small vocab | Very long sequences |
| **Word** | "hello world" â†’ ['hello','world'] | Intuitive | Huge vocab, can't handle new words |
| **Subword (BPE)** â­ | "handcrafted" â†’ ['hand','craft','ed'] | Balanced | Need understanding |

---

## ğŸ”„ BPE (Byte Pair Encoding)

Iteratively merges most frequent character pairs into new tokens.

**Example:** `"low lower newest"` â†’ `['low', 'low', 'er', 'new', 'est']`

1. Start with characters
2. Count pair frequencies
3. Merge most frequent pair
4. Repeat until vocab size reached

**Key benefit:** Handles unknown words by breaking into subwords!

---

## ğŸ”§ Tokenizer in Python

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Encode: text â†’ token IDs
encoded = tokenizer.encode("Hello world!")
# Output: [101, 7592, 2088, 999, 102]

# Decode: token IDs â†’ text
decoded = tokenizer.decode(encoded)

# Full processing
tokens = tokenizer(
    "Hello world!",
    padding="max_length",
    truncation=True,
    max_length=512,
    return_tensors="pt"
)
# Output: {'input_ids': [...], 'attention_mask': [...]}
```

---

## ğŸ¯ Model Variants

| Variant | Training | Best For | Example |
|---------|----------|----------|---------|
| **Base** | Raw text | Research, fine-tuning | "The future of AI is..." (continues) |
| **Chat** â­ | Conversations | General use, Q&A | Follows instructions, maintains context |
| **Code** | GitHub, Stack Overflow | Programming | Generates syntactically correct code |
| **Math** | Math problems | Calculations | Reasoning chains |

---

## ğŸ’¬ Chat Templates: apply_chat_template()

Different models need different formats:
```
GPT:    [{"role": "user", "content": "..."}]
Claude: Human: ... \n\nAssistant: ...
Llama:  <s>[INST] ... [/INST] ... </s>
```

**Solution:** Use `apply_chat_template()`

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat")

messages = [
    {"role": "user", "content": "What is 2+2?"}
]

prompt = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
# Automatically converts to model-specific format!
```

**Why it matters:**
- Without it: Model confused, random output âŒ
- With it: Perfect formatted response âœ…

---

## ğŸ¨ Chat Formats by Model

```
Llama 2:   <s>[INST] <<SYS>>...</SYS>> message [/INST]
Mistral:   [INST] message [/INST]
Claude:    Human: message\n\nAssistant:
GPT:       {"role": "user", "content": "message"}
```

---

## ğŸ¯ Key Concepts

| Concept | Purpose |
|---------|---------|
| **Tokenizer** | Text â†” Token IDs |
| **BPE** | Breaks words into subwords |
| **Special Tokens** | Control model behavior |
| **Base Model** | Raw model, good for research |
| **Chat Model** | Best for conversations |
| **Chat Template** | Ensures correct format |

---

## ğŸ’¡ Pro Tips

1. **Use model-specific tokenizer** - Don't mix tokenizers!
2. **Always use chat templates** - Wastes tokens & error-prone otherwise
3. **Check vocabulary size:** `tokenizer.vocab_size` (usually 25K-130K)
4. **Different models = different token IDs** - "hello" might be ID 101 or ID 7592
5. **Context window = max tokens:** 1 token â‰ˆ 0.75 words

---

## ğŸ”— Common Operations

```python
tokenizer.vocab_size          # Total tokens
tokenizer.model_max_length    # Max sequence length
tokenizer.tokenize(text)      # Text â†’ token strings
tokenizer.encode(text)        # Text â†’ token IDs
tokenizer.decode(ids)         # Token IDs â†’ text
```